{"cells":[{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["import time\n","from datasets import load_dataset, Dataset\n","import transformers\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# latest version\n","dataset = load_dataset(\"izumi-lab/llm-japanese-dataset-vanilla\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# 시간 측정을 위한 함수\n","def measure_time(func):\n","    def wrapper(*args, **kwargs):\n","        start_time = time.time()\n","        result = func(*args, **kwargs)\n","        end_time = time.time()\n","        print(f\"{func.__name__} took {end_time - start_time:.2f} seconds\")\n","        return result\n","    return wrapper"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["load_data took 2.35 seconds\n"]}],"source":["# 데이터셋 로드\n","@measure_time\n","def load_data():\n","    return load_dataset(\"izumi-lab/llm-japanese-dataset-vanilla\")\n","dataset = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'train': 2492588}\n","{'train': 3}\n","{'train': ['instruction', 'input', 'output']}\n","{'train': (2492588, 3)}\n"]}],"source":["print(dataset.num_rows)\n","print(dataset.num_columns)\n","print(dataset.column_names)\n","print(dataset.shape)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.71s/it]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","\n","# Add a padding token if it doesn't exist\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","\n","\n","# Prepare dataset for training\n","def format_dataset(example):\n","    return {\n","        \"input_ids\": tokenizer(example[\"input\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n","        \"labels\": tokenizer(example[\"output\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n","    }\n","\n","train_dataset = dataset[\"train\"].map(format_dataset, batched=True)\n","\n","# LoRA configuration\n","peft_config = LoraConfig(\n","    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",")\n","\n","# # 텍스트 생성 파이프라인 설정\n","# pipeline = transformers.pipeline(\n","#     \"text-generation\",\n","#     model=model_id,\n","#     model_kwargs={\"torch_dtype\": torch.bfloat16, \"rope_scaling\": {\"type\": \"dynamic\", \"factor\": 8.0}},\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델에 LoRA 적용\n","@measure_time\n","def apply_lora(model, config):\n","    return get_peft_model(model, config)\n","model = apply_lora(model, peft_config)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=16,\n","    learning_rate=2e-4,\n","    num_train_epochs=3,\n","    logging_steps=10,\n","    save_strategy=\"epoch\",\n","    fp16=True,\n","    push_to_hub=False\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    tokenizer=tokenizer\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델 미세 조정\n","@measure_time\n","def train_model():\n","    trainer.train()\n","\n","train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 이전과 결과 비교\n","@measure_time\n","def evaluate_model():\n","    return trainer.evaluate()\n","\n","evaluation_results = evaluate_model()\n","print(evaluation_results)"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
